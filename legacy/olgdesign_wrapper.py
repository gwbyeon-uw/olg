import argparse
import os
import sys
import warnings

import torch
import torch.nn.functional as F

import numpy as np

from translator import *
from olgdesign import *
#from ofvalidation import *

warnings.filterwarnings("ignore")

def parse_args(p):
    g_paths = p.add_argument_group('Path', 'Output and model weight paths')
    g_info = p.add_argument_group('Prefix and number', 'Prefix of design names, number of designs to create and starting number')
    g_olg = p.add_argument_group('Design parameters', 'For setting up the OLG problem')
    g_loss = p.add_argument_group('Loss function', 'Options to configure the loss function and specify other constraints')
    g_opt = p.add_argument_group('Optimizer', 'Optimization parameters')
    g_of = p.add_argument_group('Validation', 'OpenFold validation parameters')
    g_flags = p.add_argument_group('Output', 'Extra output options')
    
    g_paths.add_argument('--out', type=str, default='./out', help='Output path prefix')
    g_paths.add_argument('--weight_dir', type=str, default='./', help='Path to RoseTTAFold, OpenFold (for validation) and translator net weights')
    
    g_info.add_argument('--num_designs', type=int, default=1, help='Number of independent overlapping genes to design')
    g_info.add_argument('--prefix', type=str, default='', help='Prefix of the design names to identify them')
    g_info.add_argument('--start_num', type=int, default=0, help='Start at this design number')

    g_olg.add_argument('--no_f2', action='store_true', help='For optimizing only protein 1 as a control (protein 2 is still predicted, which provides a "background" of structures in the alternate frame)')
    g_olg.add_argument('--length_1', type=int, default=100, help='Length of protein 1')
    g_olg.add_argument('--length_2', type=int, default=100, help='Length of protein 2')
    g_olg.add_argument('--alt_frame', type=int, default=5, help='Reading frame of the second protein relative to the first. 1:+1, 2:+2, 3:-0, 4:-1, 5:-2')
    g_olg.add_argument('--offset', type=int, default=1, help='Offset between the two protein sequences (in protein sequence length units; for example, --alt_frame 5 --offset 1 will result in total 5nt shift between the two proteins (+2 from frame, +3 from 1AAx3nt offset)')
                                      
    g_loss.add_argument('--force_aa', type=str, default=None, help='Forced amino acid positions; takes a text file in tab-delimited format, where each line is: Prot Pos AA (example: 1 25 H means force histidine at 25th position of protein 1)')
    g_loss.add_argument('--last_stop', action='store_true', help='Forces stop codons at the ends of protein sequences; make sure --offset is at least 1 if this is enabled')
    g_loss.add_argument('--bkg_1', type=str, default=None, help='Background for protein 1 - if none provided, one will be generated by sampling the predictor 100 times; see mk_bkg function in olgdesign.py)')
    g_loss.add_argument('--bkg_2', type=str, default=None, help='Background for protein 2')
    g_loss.add_argument('--mask_1', type=str, default=None, help='Mask for protein 1 (as 2D L x L tensor in pytorch); if none provided, one will be generated by setting diagonals to zero and the first/last few positions as specified by --mask_offset')
    g_loss.add_argument('--mask_2', type=str, default=None, help='Mask for protein 2')
    g_loss.add_argument('--mask_offset', type=int, default=5, help='For masking out first few amino acids to give some space at the end i.e. around fixed start and stop codons (ignored if masks are specified)')
    
    g_loss.add_argument('--weight_kl_gd', type=float, default=3.0, help='Weight for KL divergence loss during gradient descent')
    g_loss.add_argument('--weight_ce_gd', type=float, default=0.0, help='Weight for cross entropy loss during gradient descent; leave at 0.0 - currently calculated but should not be used for loss')
    g_loss.add_argument('--weight_lddt_gd', type=float, default=0.0, help='Weight for LDDT loss during gradient descent; leave at 0.0 - currently calculated but should not be used for loss')
    g_loss.add_argument('--weight_stop_gd', type=float, default=2.0, help='Weight for stop codon loss (this penalizes appearances of stop codons in the middle of ORF during gradient descent')
    g_loss.add_argument('--weight_force_gd', type=float, default=2.0, help='Weight for loss to force fixed amino acid positions during gradient descent')
    g_loss.add_argument('--weight_last_gd', type=float, default=2.0, help='Weight for loss to force stop codon at the end during gradient descent')
    g_loss.add_argument('--weight_rog_gd', type=float, default=0.5, help='Weight for radius of gyration loss during gradient descent')
    g_loss.add_argument('--weight_rog_sum_gd', type=float, default=0.0, help='Weight for sum of radius of gyration loss during gradient descent')
    g_loss.add_argument('--thres_rog_gd', type=float, default=16.0, help='Threshold for radius of gyration loss during gradient descent')
    g_loss.add_argument('--thres_rog_sum_gd', type=float, default=40.0, help='Threshold for sum of radius of gyration loss during gradient descent')
    g_loss.add_argument('--weight_surfnp_gd', type=float, default=0.0, help='Weight for surface non-polar loss during gradient descent')
    g_loss.add_argument('--thres_surfnp_gd', type=float, default=1.0, help='Threshold for surface non-polar loss during gradient descent')
    g_loss.add_argument('--weight_nc_gd', type=float, default=0.0, help='Weight for negative net charge loss during gradient descent')
    g_loss.add_argument('--thres_nc_gd', type=float, default=0.0, help='Threshold for negative net charge loss during gradient descent')
                                      
    g_loss.add_argument('--weight_kl_sa', type=float, default=3.0, help='Weight for KL divergence loss during simulated annealing')
    g_loss.add_argument('--weight_ce_sa', type=float, default=0.0, help='Weight for cross entropy loss during simulated annealing; leave at 0.0 - currently calculated but should not be used for loss')
    g_loss.add_argument('--weight_lddt_sa', type=float, default=0.0, help='Weight for LDDT loss during simulated annealing; leave at 0.0 - currently calculated but should not be used for loss')
    g_loss.add_argument('--weight_stop_sa', type=float, default=1.0, help='Weight for stop codon loss during simulated annealing (this penalizes appearances of stop codons in the middle of ORF')
    g_loss.add_argument('--weight_force_sa', type=float, default=2.0, help='Weight for loss to force fixed amino acid positions during simulated annealing')
    g_loss.add_argument('--weight_last_sa', type=float, default=2.0, help='Weight for loss to force stop codon at the end during simulated annealing')
    g_loss.add_argument('--weight_rog_sa', type=float, default=1.0, help='Weight for radius of gyration loss during simulated annealing')
    g_loss.add_argument('--weight_rog_sum_sa', type=float, default=0.0, help='Weight for sum of radius of gyration during simulated annealing')
    g_loss.add_argument('--thres_rog_sa', type=float, default=16.0, help='Threshold for radius of gyration loss during simulated annealing')
    g_loss.add_argument('--thres_rog_sum_sa', type=float, default=40.0, help='Threshold for sum of radius of gyration loss during simulated annealing')
    g_loss.add_argument('--weight_surfnp_sa', type=float, default=1.0, help='Weight for surface non-polar loss during simulated annealing')
    g_loss.add_argument('--thres_surfnp_sa', type=float, default=0.2, help='Threshold for surface non-polar loss during simulated annealing')
    g_loss.add_argument('--weight_nc_sa', type=float, default=0.1, help='Weight for negative net charge loss during simulated annealing')
    g_loss.add_argument('--thres_nc_sa', type=float, default=-1.0, help='Threshold for negative net charge loss during simulated annealing')
    g_loss.add_argument('--max_alpha_gd', type=float, default=0.4, help='Maximum penalty for large differences in loss between the two proteins during gradient descent')
    g_loss.add_argument('--max_alpha_sa', type=float, default=0.4, help='Maximum penalty for large differences in loss between the two proteins during simulated annealing')
    g_loss.add_argument('--kl_alpha_gd', type=float, default=0.7, help='If the worse KL divergence is better than this value, then do not apply the penalty for large difference in loss between the two proteins during gradient descent')
    g_loss.add_argument('--kl_alpha_sa', type=float, default=1.1, help='If the worse KL divergence is better than this value, then do not apply the penalty for large difference in loss between the two proteins during simulated annealing')
    g_loss.add_argument('--max_aloss_gd', type=float, default=0.05, help='Maximum auxilary loss; this clips the contribution of ROG, surface non-polar and negative net charge losses during gradient descent')
    g_loss.add_argument('--max_aloss_sa', type=float, default=1e8, help='Maximum auxilary loss; this clips the contribution of ROG, surface non-polar and negative net charge losses during simulated annealing')
                 
    g_opt.add_argument('--device', type=str, default='cuda:0', help='Device to run (currently can be *ONLY* CUDA)')
    g_opt.add_argument('--starting_weights', type=str, default=None, help='Starting weights; randomly initialized with Xavier normal if none provided')
    g_opt.add_argument('--lr', type=float, default=0.05, help='Learning rate for the inner gradient descent optimizer (ADAM)')
    g_opt.add_argument('--beta_1', type=float, default=0.5, help='Beta 1 for ADAM')
    g_opt.add_argument('--beta_2', type=float, default=0.5, help='Beta 2 for ADAM')
    g_opt.add_argument('--eps', type=float, default=1e-3, help='Epsilon for ADAM')
    g_opt.add_argument('--lookahead_k', type=int, default=5, help='Lookahead outer optimizer k; set this to 1 and --lookahead_a to 1.0 at the same time to disable Lookahead')
    g_opt.add_argument('--lookahead_a', type=float, default=0.5, help='Lookahead outer optimizer alpha; set this to 1.0 and --lookahead_k to 1 at the same time to disable Lookahead')
    g_opt.add_argument('--gd_steps', type=int, default=1000, help='Total number of gradient descent iterations to run')
    g_opt.add_argument('--max_gd_steps', type=int, default=5000, help='Total number of gradient descent iterations (including resets)')
    g_opt.add_argument('--reset_kl', type=float, default=0.35, help='If average KL divergence of the two proteins are less than this value after specified number of steps, then the weights are reinitialized and optimization is restarted')
    g_opt.add_argument('--reset_steps', type=int, default=100, help='The number of steps after which the weights will be reinitialized if specified KL divergence metric has not been reached')
    g_opt.add_argument('--reset_kl_final', type=float, default=0.65, help='If average KL divergence of the two proteins are less than this value at the end of gradient descent, the weights are reinitialized and optimization is restarted')
    g_opt.add_argument('--gd_hcap', type=float, default=0.7, help='If KL loss for both proteins are better than this value AND the solution is not acceptable due to hard constraints such as presence of stop codons, hallucination gradient will be zero\'ed out')
    g_opt.add_argument('--gd_hcap_max', type=int, default=4, help='Number of gradient descent iterations for which hallucination loss will be zero\'ed out following an unacceptable solution')
    g_opt.add_argument('--early_gd_stop', type=float, default=1.1, help='Hard early stop condition for gradient descent; reaching KL loss better than this value for both proteins will terminate gradient descent loop')
    g_opt.add_argument('--plateau_gd', type=float, default=0.05, help='Threshold to detect plateau during gradient descent; if average KL diverence does not improve by this value within specified number of steps during gradient descent, then early stop, reset or learning rate decay is triggered')
    g_opt.add_argument('--patience_gd', type=float, default=200, help='Number of steps to wait for improvement in average KL divergence to detect plateau during gradient descent')
    g_opt.add_argument('--decay_min_lr', type=float, default=0.01, help='Minimum allowed learning rate after decay during gradient descent; falling below this value triggers early stop')
    g_opt.add_argument('--decay_factor_lr', type=float, default=1.0, help='Learning rate decay factor (exponential decay)')
    g_opt.add_argument('--sa_steps', type=int, default=5000, help='Total number of simulated annealing iterations')
    g_opt.add_argument('--min_sa_steps', type=int, default=500, help='Minimum number of simulated annealing iterations before early stop criterion can be applied')
    g_opt.add_argument('--max_mut', type=int, default=1, help='Maximum number of nucleotides to mutate in one iteration of simulated annealing')
    g_opt.add_argument('--init_tau', type=float, default=5e-3, help='Initial temperature for simulated annealing')
    g_opt.add_argument('--anneal_rate', type=float, default=1e-5, help='Temperature decrement per each iteration of simulated annealing')
    g_opt.add_argument('--min_tau', type=float, default=1e-5, help='Minimum temperature for simulated annealing')
    g_opt.add_argument('--plateau_sa', type=float, default=0.05, help='Threshold to detect plateau during simulated annealing; if average KL divergence for two proteins do not improve by this number within specified number of steps, then simulated annealing is terminated')
    g_opt.add_argument('--patience_sa', type=int, default=500, help='Number of steps to wait for improvement in average KL divergence to detect plateau simulated annealing iterations')
    
    g_of.add_argument('--openfold', action='store_true', help='Enables running OpenFold predictions for the best solution as an orthogonal validation')
    g_of.add_argument('--max_recycle', type=int, default=12, help='Maximum number of recycles to use in OpenFold prediction (this sometimes significantly increases prediction accuracy)')
                                      
    g_flags.add_argument('--out_anim', action='store_true', help='Enables creating an animation of the optimization run; this can take a while (10-15 minutes for ~1000 total steps)')
    g_flags.add_argument('--print_loss', action='store_true', help='Enables printing the current loss during optimization')
    
    args = p.parse_args()
    return args
    
def main(args):
    nucleotides = ['A', 'T', 'G', 'C']
    amino_acids = list("ARNDCQEGHILKMFPSTWYV*")
    
    device = args.device    
    f1_length = args.length_1
    f2_length = args.length_2
    f1_frame = 0 #Always zero
    f2_frame = args.alt_frame
    offset = args.offset    
    total_length = max(f1_length, f2_length) + offset + 2 #Length of input nucleotide sequence
    num_designs = args.num_designs
    design_prefix = args.prefix
    start_num = args.start_num

    if not args.last_stop:
        last_weight_gd = 0.0
        last_weight_sa = 0.0
    else:
        last_weight_gd = args.weight_last_gd
        last_weight_sa = args.weight_last_sa
    
    #For fixing AAs at specific positions
    f1_force = torch.zeros(1, 21, f1_length)
    f2_force = torch.zeros(1, 21, f2_length)
    force_aa = [ f1_force, f2_force ]
    if args.force_aa is not None:
        with open(args.force_aa) as force_aa_file:
            for line in force_aa_file:
                l = line.split('\t')
                prot = int(l[0]) - 1
                pos = int(l[1]) - 1
                aa = amino_acids.index(l[2].rstrip())
                force_aa[prot][:, aa, pos] = 1.0

    #Mask for KL divergence or cross entropy loss            
    if args.mask_1 is not None:
        mask_f1 = torch.load(args.mask_f1).unsqueeze(0)        
    else:
        wstart = args.mask_offset #Mask out first few AAs to give some space for fixed AAs and stop codons
        wend = f1_length - args.mask_offset
        mask_f1 = torch.zeros(f1_length, f1_length)
        mask_f1[wstart:wend, wstart:wend].fill_(1.0)
        mask_f1.fill_diagonal_(0.0)
        mask_f1 = mask_f1.unsqueeze(0)
    
    #Mask for protein 2
    if args.mask_2 is not None:
        mask_f2 = torch.load(args.mask_f2).unsqueeze(0)
    else:
        wstart = args.mask_offset
        wend = f2_length - args.mask_offset
        mask_f2 = torch.zeros(f2_length, f2_length)
        mask_f2[wstart:wend, wstart:wend].fill_(1.0)
        mask_f2.fill_diagonal_(0.0)
        mask_f2 = mask_f2.unsqueeze(0)
    
    #Make or load background for KLD loss
    mk_bkg_f1 = False
    if args.bkg_1 is not None:
        bkg_1 = torch.load(args.bkg_1)
    else:
        mk_bkg_f1 = True
        
    #Background for protein 2
    mk_bkg_f2 = False
    if args.bkg_2 is not None:
        bkg_2 = torch.load(args.bkg_2)
    else:
        mk_bkg_f2 = True
    
    outdir = os.path.abspath(args.out) #Remember abs output path
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    
    if args.print_loss:
        for k, v in args.__dict__.items():
            print(f'{k:<20}{v}')
        print('---')
    
    #Load rosettafold net
    os.chdir(os.path.abspath(args.weight_dir))
    sys.path.insert(0, "./util")
    include_dir = './'
    network_name = 'rf_Nov05_2021'
    #network_name = 'rf_Jul31_2022'
    weights_dir = "./weights"
    rosetta, rosetta_params = load_model(include_dir, network_name, weights_dir, device)
    
    #Load translator net
    translator_file = "./weights/translator/translator_cnn_1024ch.pth"
    translator = torch.load(translator_file)
    
    if mk_bkg_f1:
        bkg_1 = mk_bkg(rosetta, f1_length, device, n_runs=1000)
        torch.save(bkg_1, outdir + "/bkg_1.pth")
    
    if mk_bkg_f2:
        bkg_2 = mk_bkg(rosetta, f2_length, device, n_runs=1000)
        torch.save(bkg_2, outdir + "/bkg_2.pth")
    
    #Load starting weights if provided
    if args.starting_weights is not None:
        starting_weights = torch.load(args.starting_weights)
    else:
        starting_weights = None
    
    for n in range(num_designs):
        
        if design_prefix == '':
            design_name = str(start_num + n)
        else:
            design_name = design_prefix + '_' + str(start_num + n) 

        print(f"\nRunning main design loop for design {n+1}/{num_designs}..\n")
        result = run_design(args.device, rosetta, translator,
                        network_name,
                        total_length, f1_frame, f2_frame, offset,
                        f1_force, f2_force, bkg_1, bkg_2, mask_f1, mask_f2, 
                        args.last_stop, args.no_f2,
                        lr=args.lr, betas=(args.beta_1, args.beta_2), 
                        eps=args.eps, 
                        lookahead_k=args.lookahead_k, lookahead_alpha=args.lookahead_a,
                        n_step_gd=args.gd_steps, 
                        n_step_gd_n=args.gd_hcap,
                        n_max_h=args.gd_hcap_max,
                        early_gd_stop=args.early_gd_stop,
                        reset_kl=args.reset_kl, 
                        reset_step=args.reset_steps,
                        max_total_step=args.max_gd_steps,    
                        n_step_sa=args.sa_steps,
                        alpha_gd=args.max_alpha_gd, alpha_sa=args.max_alpha_sa,
                        weight_kl_gd=args.weight_kl_gd, 
                        weight_ce_gd=args.weight_ce_gd,
                        weight_lddt_gd=args.weight_lddt_gd,
                        weight_stop_gd=args.weight_stop_gd, 
                        weight_force_gd=args.weight_force_gd, 
                        weight_last_gd=last_weight_gd,
                        weight_rog_gd=args.weight_rog_gd,
                        weight_rog_sum_gd=args.weight_rog_sum_gd,
                        rog_thres_gd=args.thres_rog_gd, 
                        rog_sum_thres_gd=args.thres_rog_sum_gd, 
                        weight_surfnp_gd=args.weight_surfnp_gd,
                        surfnp_thres_gd=args.thres_surfnp_gd,
                        weight_nc_gd=args.weight_nc_gd,
                        nc_thres_gd=args.thres_nc_gd,
                        weight_kl_sa=args.weight_kl_sa, 
                        weight_ce_sa=args.weight_ce_sa,
                        weight_lddt_sa=args.weight_lddt_sa,
                        weight_stop_sa=args.weight_stop_sa, 
                        weight_force_sa=args.weight_force_sa, 
                        weight_last_sa=last_weight_sa,
                        weight_rog_sa=args.weight_rog_sa,
                        weight_rog_sum_sa=args.weight_rog_sum_sa,
                        rog_thres_sa=args.thres_rog_sa, 
                        rog_sum_thres_sa=args.thres_rog_sum_sa, 
                        weight_surfnp_sa=args.weight_surfnp_sa,
                        surfnp_thres_sa=args.thres_surfnp_sa,
                        weight_nc_sa=args.weight_nc_sa,
                        nc_thres_sa=args.thres_nc_sa,
                        max_mut=args.max_mut, 
                        tau0=args.init_tau,
                        anneal_rate=args.anneal_rate, 
                        min_temp=args.min_tau, 
                        start_weights=starting_weights,
                        decay_kl_threshold=args.plateau_gd, 
                        decay_min_lr=args.decay_min_lr, 
                        decay_patience=args.patience_gd, 
                        decay_factor_lr=args.decay_factor_lr,
                        min_steps=args.min_sa_steps, 
                        patience=args.patience_sa, 
                        early_stop_thres=args.plateau_sa,
                        aloss_max_gd=args.max_aloss_gd,
                        aloss_max_sa=args.max_aloss_sa,
                        reset_kl_final=args.reset_kl_final,
                        kl_alpha_gd=args.kl_alpha_gd,
                        kl_alpha_sa=args.kl_alpha_sa,
                        print_loss=args.print_loss)
    
        #Save the trajectory
        trk_dump_file = outdir + f"/{design_name}_trajectory.pth"
        print("Dumping the optimization trajectory: " + trk_dump_file)
        torch.save(result, trk_dump_file)
        
        #Plot animation of optimization run
        if args.out_anim:
            trk_anim_file = outdir + f"/{design_name}_trajectory.gif"
            print("Outputting an animation of the optimization trajectory: " + trk_anim_file)
            plot_res = anim_res(result, 1, 20)
            plot_res.save(trk_anim_file)
        
        #Output best solution
        best_soln_file = outdir + f"/{design_name}_best_solution.seq"
        print("Writing the best solution: " + best_soln_file)
        min_prot_1 = ''.join([amino_acids[i] for i in torch.argmax(result[1]['seq'][result[1]['min_step'][-1]]['prot'][0], 1)[0]])
        min_prot_2 = ''.join([amino_acids[i] for i in torch.argmax(result[1]['seq'][result[1]['min_step'][-1]]['prot'][1], 1)[0]])
        trim_nt = (f2_frame % 3) - 3 #Need to trim 1~3nt depending on frame
        min_nuc = ''.join([nucleotides[i] for i in torch.argmax(result[1]['seq'][result[1]['min_step'][-1]]['nuc'], 1)[0]])[:trim_nt]
        
        with open(best_soln_file, 'w') as outfile:
            outfile.write(min_prot_1+'\n')
            outfile.write(min_prot_2+'\n')
            outfile.write(min_nuc)
        
        #Plot distogram and LDDT of final solution
        best_soln_plot_file = outdir + f"/{design_name}_best_solution.svg"
        print("Outputting a plot of the best solution: " + best_soln_plot_file)
        best_soln_plot = min_dist(result)
        best_soln_plot.savefig(best_soln_plot_file)
        
        if args.openfold:
            print("Running OpenFold validation..")
            
            del result
            del rosetta
            del translator
            torch.cuda.empty_cache()

            openfold_weight_path = "./weights/openfold/"
            
            min_of_result = [ None, None ]
            min_of_result_model = [ None, None ]
            for model in [ 'model_1', 'model_2', 'model_3', 'model_4', 'model_5' ]:
                print("OpenFold " + model + "..")
                of_model, of_cfg = load_openfold(openfold_weight_path, args.max_recycle, model)

                f1_of_input, f1_fd = prepare_openfold_input(min_prot_1, of_cfg)
                if not args.no_f2:
                    f2_of_input, f2_fd = prepare_openfold_input(min_prot_2, of_cfg)

                with torch.no_grad():
                    f1_of_out = of_model(f1_of_input)
                    if not args.no_f2:
                        f2_of_out = of_model(f2_of_input)
                    else:
                        f2_of_out = None

                of_dump_file = outdir + f"/{design_name}_openfold_dump_" + model + ".pth"
                print("Dumping OpenFold predictions (" + model + "): " + of_dump_file)
                torch.save((f1_of_out, f2_of_out), of_dump_file)

                of_plot_file = outdir + f"/{design_name}_openfold_summary_" + model + ".svg"
                print("Outputting a plot of OpenFold predictions (" + model + "): " + of_plot_file)
                of_fig = plot_2d(f1_of_out, f2_of_out)
                of_fig.savefig(of_plot_file)

                f1_unrelaxed_pdb = get_pdb(f1_of_input, f1_of_out, f1_fd, of_cfg)
                f1_pdb_file = outdir + f"/{design_name}_openfold_p1_" + model + ".pdb"
                if not args.no_f2:
                    f2_unrelaxed_pdb = get_pdb(f2_of_input, f2_of_out, f2_fd, of_cfg)
                    f2_pdb_file = outdir + f"/{design_name}_openfold_p2_" + model + ".pdb"
                    print("Writing PDB (unrelaxed) of OpenFold predictions (" + model + "): " + f1_pdb_file + " / " + f2_pdb_file)
                else:
                    f2_unrelaxed_pdb = None
                    f2_pdb_file = ""
                    print("Writing PDB (unrelaxed) of OpenFold predictions (" + model + "): " + f1_pdb_file)

                with open(f1_pdb_file, 'w') as f1_fp:
                    f1_fp.write(f1_unrelaxed_pdb)
                if not args.no_f2:
                    with open(f2_pdb_file, 'w') as f2_fp:
                        f2_fp.write(f2_unrelaxed_pdb)

                if min_of_result_model[0] is None:
                    min_of_result[0] = f1_of_out
                    min_of_result_model[0] = model
                    if not args.no_f2:
                        min_of_result[1] = f2_of_out
                        min_of_result_model[1] = model
                else:
                    if torch.mean(f1_of_out['plddt']) > torch.mean(min_of_result[0]['plddt']):
                        min_of_result[0] = f1_of_out
                        min_of_result_model[0] = model
                    if not args.no_f2:
                        if torch.mean(f2_of_out['plddt']) > torch.mean(min_of_result[1]['plddt']):
                            min_of_result[1] = f2_of_out
                            min_of_result_model[1] = model

            if not args.no_f2:
                of_plot_file = outdir + f"/{design_name}_openfold_summary_top_" + min_of_result_model[0] + "_" + min_of_result_model[1] + ".svg"
            else:
                of_plot_file = outdir + f"/{design_name}_openfold_summary_top_" + min_of_result_model[0] + ".svg"
            print("Outputting a plot of OpenFold predictions (top): " + of_plot_file)
            of_fig = plot_2d(min_of_result[0], min_of_result[1])
            of_fig.savefig(of_plot_file)

    print("Finished!")
        
if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="olgdesign_wrapper.py",
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    args = parse_args(parser)
    if len(sys.argv)==1:
        parser.print_help(sys.stderr)
        sys.exit(1)
    main(args)
